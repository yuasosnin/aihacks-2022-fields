{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cd410d-6a06-4779-8942-4646e0c8f340",
   "metadata": {},
   "source": [
    "This notebook provides code for downloading additional data form MODIS, Landsat and Sentinel-2, using Google Earth Engine Python API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d993b0d-11e7-4392-a0c8-9a12bcca0637",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd409f00-3019-4875-b9b5-af40baa3243f",
   "metadata": {},
   "source": [
    "Firstly, import libraries and read data provided by the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a31d07-861a-404a-a538-0145c26992b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from src import read_data, process_data, split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82fc4aa-1966-4689-a393-981b3823e585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = read_data('data/train_dataset_train_2.csv')\n",
    "data_test = read_data('data/test_dataset_test_2.csv')\n",
    "data_cat = pd.concat((data, data_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedc91e-19e1-4079-af1b-61f1cacf8293",
   "metadata": {},
   "source": [
    "Authenticate Google Earth Engine. If you are registered there, it will provide an API key to authenticate this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a875be-b323-499b-b28f-812a6feb376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import eeconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c635e10-b95b-4cf6-a520-94e80f18e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d32d3d-6705-4a1f-a74d-45594391c1f4",
   "metadata": {},
   "source": [
    "Now we define the primary funciton that will fetch time series data from Google Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8613f0c6-f0ad-47a7-a0e6-2e8feac4b692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_series(collection, features, band='ndvi', scale=30, drive_folder=None):\n",
    "    \"\"\"https://stackoverflow.com/questions/47633088/get-results-in-an-earth-engine-python-script\"\"\"\n",
    "    def GetSeries(feature):\n",
    "        def NDVIcalc(img):\n",
    "            return feature.set(\n",
    "                img.reduceRegion(ee.Reducer.median(), feature.geometry(), scale)\n",
    "            ).set('date', img.date().format('YYYY-MM-dd'))\n",
    "\n",
    "        series = collection.map(NDVIcalc)\n",
    "\n",
    "        lst = series.reduceColumns(\n",
    "            ee.Reducer.toList(2), ['date', band.upper()]).get('list')\n",
    "        return feature.set(ee.Dictionary(ee.List(lst).flatten()))\n",
    "\n",
    "    features_mapped = features.map(GetSeries)\n",
    "    if drive_folder is None:\n",
    "        result = features_mapped.getInfo()\n",
    "        return pd.DataFrame([i['properties'] for i in result['features']])\n",
    "    else:\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            features_mapped, folder=drive_folder, fileFormat='csv')\n",
    "        task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672be077-be38-4d4f-a901-2f21378a093d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Donwnloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158bac2-185e-4018-8302-f6c8670f55db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Landsat-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565174e2-ccf7-45ae-a6f6-3e41057d3b39",
   "metadata": {},
   "source": [
    "First, split data into tiles, corresponding to Landsat scenes. This is done beacause otherwise each API call would be too large and get timed out by the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140e4b5c-8ac3-4df6-93ff-a8a15175cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid source: https://mgimond.github.io/ArcGIS_tutorials/Download_landsat.htm\n",
    "grid = gpd.read_file('data/grid/wrs2.shp')\n",
    "data_grid = data_cat.sjoin(grid[['PATH', 'ROW', 'geometry']], how='left')\n",
    "gb = data_grid.groupby(['PATH', 'ROW'])\n",
    "tiles = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6d7c0-c09c-4c07-a9c6-26915e2dc23c",
   "metadata": {},
   "source": [
    "Donwnload time series directly with the funciton defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7177b8-f28d-4534-9307-4011e225ea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a25331570164c3890dfba86c338313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop over tiles to get series df for them from GEE\n",
    "out = []\n",
    "for tile in tqdm(tiles):\n",
    "    # convert gpd geometry to ee.FeatureCollection\n",
    "    features = eeconvert.gdfToFc(tile.to_crs(4326)[['id', 'geometry']])\n",
    "    # define ee.ImageCollection of desired sattelite\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_8DAY_NDVI') \\\n",
    "        .filterBounds(features) \\\n",
    "        .filterDate('2021-04-15', '2021-09-01') \\\n",
    "        .select('NDVI')\n",
    "    series = get_series(collection, features, scale=30)\n",
    "    out.append(series)\n",
    "data_landsat = pd.concat(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ca09c-cea7-41a0-8111-627238446d51",
   "metadata": {},
   "source": [
    "Group results by id and select maximum values in case of duplicates (scene overlapping). We choose max is better bacause in one of the images there can be cluods, so max value (possibly without coluds) is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0fb16-9fee-4a89-815b-0ad90f5e0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_landsat = data_landsat.groupby('id').max().reset_index(drop=True)\n",
    "data_landsat = data_landsat.loc[data_cat['id'], sorted(data_landsat.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b0003-fdf7-4794-8f95-e35f9dbcd4dd",
   "metadata": {},
   "source": [
    "Save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506a4743-2c27-4358-8e27-fe2f4137ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_landsat.iloc[:data.shape[0],:].to_csv('data/train_dataset_landsat.csv', index=False)\n",
    "data_landsat.iloc[data.shape[0]:,:].to_csv('data/test_dataset_landsat.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351eb8f-bbf0-492d-a354-03652c063037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MODIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df812b7-36c9-474e-94f8-5764c971ad78",
   "metadata": {},
   "source": [
    "First, split data into tiles, corresponding to MODIS scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a73799-e6b4-429a-a1fa-ed21c616c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid source: https://mgimond.github.io/ArcGIS_tutorials/Download_landsat.htm\n",
    "grid = gpd.read_file('data/grid/modis_sinusoidal_grid_world.shp')\n",
    "data_grid = data_cat.to_crs(grid.crs).sjoin(grid[['h', 'v', 'geometry']], how='left')\n",
    "gb = data_grid.groupby(['h', 'v'])\n",
    "tiles = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ee07-fc9a-4dfc-90c3-2198c6532761",
   "metadata": {},
   "source": [
    "Donwnload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a1e16b-9f33-403d-886f-92433df60bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db54266f90784593aed4888c0192ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split tiles further into even smaller dataframes of size <= 100\n",
    "tiles_gen = (t for tile in tiles for t in split_df(tile, 100))\n",
    "\n",
    "# loop over tiles to get series df for them from GEE\n",
    "out = []\n",
    "for tile in tqdm(tiles_gen):\n",
    "    # convert gpd geometry to ee.FeatureCollection\n",
    "    features = eeconvert.gdfToFc(tile.to_crs(4326)[['id', 'geometry']])\n",
    "    # define ee.ImageCollection of desired sattelite\n",
    "    collection = ee.ImageCollection('MODIS/MOD09GA_006_NDVI') \\\n",
    "        .filterBounds(features) \\\n",
    "        .filterDate('2021-04-15', '2021-09-01') \\\n",
    "        .select('NDVI')\n",
    "    series = get_series(collection, features, scale=250)\n",
    "    out.append(series)\n",
    "data_modis = pd.concat(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed3786-60f6-462d-ab8b-06b5f0cccc58",
   "metadata": {},
   "source": [
    "Get rid of duplicates and sort date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177ea5b8-286e-4ff7-826f-fb84ef0edc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modis = data_modis.groupby('id').max().reset_index(drop=True).fillna(0)\n",
    "data_modis = data_modis.loc[data_cat['id'], sorted(data_modis.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a06ae8-f9bf-46a1-8406-d760c25eb9ca",
   "metadata": {},
   "source": [
    "Save the data to local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3ccfdd3-bfc3-4284-9cc9-36388892e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modis.iloc[:data.shape[0],:].to_csv('data/train_dataset_modis.csv', index=False)\n",
    "data_modis.iloc[data.shape[0]:,:].to_csv('data/test_dataset_modis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551ea3a-a2e1-4407-95c0-4a269e159305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b712416d-9b27-416e-a254-37b095c96808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sentinel_ndvi(image):\n",
    "    red = image.select('B4')\n",
    "    nir = image.select('B8')\n",
    "    ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI')\n",
    "    return image.addBands(ndvi)\n",
    "\n",
    "def set_sentinel_evi(image):\n",
    "    evi = image.expression(\n",
    "        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', \n",
    "        {'NIR': image.select('B8').divide(10000),\n",
    "         'RED': image.select('B4').divide(10000),\n",
    "         'BLUE': image.select('B2').divide(10000)}\n",
    "    ).rename('EVI')\n",
    "    return image.addBands(evi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1b597-935a-45d3-8d83-214452857a2e",
   "metadata": {},
   "source": [
    "First, split data into tiles, corresponding to Sentinel-2 scenes. This is done beacause otherwise each API call would be too large and get timed out by the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92fcfd2-78f6-47da-8e05-63315f5f8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid source: https://github.com/justinelliotmeyers/Sentinel-2-Shapefile-Index\n",
    "grid = gpd.read_file('data/grid/sentinel_2_index_shapefile.shp')\n",
    "data_grid = data_cat.to_crs(grid.crs).sjoin(grid[['Name', 'geometry']], how='left')\n",
    "gb = data_grid.groupby('Name')\n",
    "tiles = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bd5db-bb57-4112-84f0-13d241c1e4f5",
   "metadata": {},
   "source": [
    "For Sentinel-2, save results into Google Drive instead of downloading them directly.\n",
    "Because Sentinel data is too ehavy, direct download would stall out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3182e7-928c-4ab0-b005-9569b466bd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6543b4ab93e64725968457f4543670f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split tiles further into even smaller dataframes of size <= 100\n",
    "tiles_gen = (t for tile in tiles for t in split_df(tile, 100))\n",
    "\n",
    "# loop over tiles to get series df for them from GEE\n",
    "for tile in tqdm(tiles_gen):\n",
    "    # convert gpd geometry to ee.FeatureCollection\n",
    "    features = eeconvert.gdfToFc(tile.to_crs(4326)[['id', 'geometry']])\n",
    "    # define ee.ImageCollection of desired sattelite\n",
    "    collection = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "        .map(set_sentinel_ndvi) \\\n",
    "        .filterBounds(features) \\\n",
    "        .filterDate('2021-04-15', '2021-09-01') \\\n",
    "        .select('NDVI')\n",
    "    # save results into Google Drive\n",
    "    get_series(collection, features, scale=10, drive_folder='sentinel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6b673-8cbc-4006-8bf0-82f5f6e5d37e",
   "metadata": {},
   "source": [
    "Processing and donwnloading to Google Drive can be monitored here:\n",
    "https://code.earthengine.google.com/tasks\n",
    "\n",
    "After the process is finished, download the folder from Google Drive manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "cc945e56-18a6-4309-82bd-29407b2e496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'sentinel'\n",
    "out = []\n",
    "for file_name in os.listdir(dir_path):\n",
    "    out.append(pd.read_csv(dir_path+'/'+file_name))\n",
    "data_sentinel = pd.concat(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864cf75f-73b8-4f4c-a46c-e4afcd4cbd01",
   "metadata": {},
   "source": [
    "Sentinel satellite passes through our regin in midnight, thus producing data with different dates.\n",
    "Fix this by mapping everything to the date before midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "bda865c8-45d0-45f5-a770-1b6e67cbb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prev = data_sentinel.iloc[:,1:56]\n",
    "data_prev.columns = pd.to_datetime(data_prev.columns)\n",
    "data_next = data_sentinel.iloc[:,59:]\n",
    "data_next.columns = pd.to_datetime(data_next.columns) - pd.Timedelta(days=1)\n",
    "data_prev.update(data_next)\n",
    "data_prev['id'] = data_sentinel['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e25e5-4430-4b45-a308-76f2120d2b27",
   "metadata": {},
   "source": [
    "Group results by id and select maximum values in case of duplicates (scene overlapping).\n",
    "We choose max is better bacause in one of the images there can be cluods, so max value (possibly without coluds) is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "927d4c7a-3fb0-448e-95a8-cd4874691d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentinel = data_prev.groupby('id').max().reset_index(drop=True).fillna(0)\n",
    "data_sentinel = data_sentinel.loc[data_cat['id'], sorted(data_sentinel.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20beef-962b-4da2-927c-95b1239a882e",
   "metadata": {},
   "source": [
    "Finally, save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "677c31d2-af62-4d46-b0da-1212f461852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentinel.iloc[:data.shape[0],:].to_csv('data/train_dataset_sentinel.csv', index=False)\n",
    "data_sentinel.iloc[data.shape[0]:,:].to_csv('data/test_dataset_sentinel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275c1ec-fce4-4e83-915c-78c362316176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
